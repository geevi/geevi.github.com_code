--- 
layout: post
title: On a Search Project
published: true
meta: 
  _edit_last: "5862988"
  blogger_author: Girish R Vhttp://www.blogger.com/profile/12960293260785652534noreply@blogger.com
  blogger_blog: girishrv.blogspot.com
  blogger_permalink: /2007/06/on-search-project.html
tags: 
- computer science
- technology
type: post
status: publish
---
I was thinking of what to do for my final year project. I had thought of a wide range of things. Being in google i was thinking of doing a search engine itself. Things are pretty simple. There should be a script which recursively parses html pages and stores them locally. Some other proccess should index them in some form. Another process should take queries from user and find out the relevant index to use. There should be some difference from what the guys from stanford did. They concentrated on getting relevance. I will try to get results keeping some other factor. So what would be the other factors. Seems like there are no other factors. The whole point is to get the relavant info. So the solution itself is getting the relavence. Another thing i can do is to use some other techniques in data mining and ai may be neural networks, pattern classification etc.. I have not much idea what these are. They sound like the ones will be of use. Have to get more info on them.

Before going through them i just thought of thinking about a solution myself. So i will have a big set of html pages and i will have to tag them with what each has relavent info on. May be even give a rating for each tag. This whole process of finding out tags from the html junk code will be really difficult. May be thats why the google guys considered also other docs which link to it and find the tag from what they have in common.  I can do the same or i was thinking of something else.

What if i have the kewords before parsing through the docs? Ofcourse the user will get the results only after a long time spent on parsing all the pages. Consider a use case of a person with a desktop and he has lets say many html pages on his lan or locally. He want to find all info on something using the limited computing he has. So i can gather all docs which has the relavent keyword, consider parts of docs( such as the title or meta tags or words closer to the keyword) and compile the results page. May be i can even give him a dependency graph in flash(just to make things look good). I guess i can give something more than what a basic find does.
